# Training

## [Distributed and Parallel Training Tutorials](https://pytorch.org/tutorials/distributed/home.html)

CODED

## [PyTorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)

CODED

## [Writing Distributed Applications with PyTorch](https://pytorch.org/tutorials/intermediate/dist_tuto.html)

CODED

## [Single-Machine Model Parallel Best Practices](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html)

CODED

## DistributedDataParallel (DDP)

### [Saving and Loading Models](https://pytorch.org/tutorials/beginner/saving_loading_models.html)

CODED

### [Getting Started with Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)

CODED

### [Shard Optimizer States with ZeroRedundancyOptimizer](https://pytorch.org/tutorials/advanced/generic_join.html)

CODED

### [Distributed Training with Uneven Inputs Using the Join Context Manager](https://pytorch.org/tutorials/advanced/generic_join.html)

CODED

### [Data Parallelism](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)

CODED

### [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)

CODED

### [DistributedDataParallel Notes](https://pytorch.org/docs/master/notes/ddp.html)

CODED

## Fully Sharded Data Parallel (FSDP)

### [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)

### [Getting Started with Fully Sharded Data Parallel(FSDP)](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)

### [Advanced Model Training with Fully Sharded Data Parallel (FSDP)](https://pytorch.org/tutorials/intermediate/FSDP_adavnced_tutorial.html)

### [torchrun (Elastic Launch)](https://pytorch.org/docs/stable/elastic/run.html)

### [FullyShardedDataParallel](https://pytorch.org/docs/stable/fsdp.html)

### [FSDP Notes](https://pytorch.org/docs/stable/notes/fsdp.html#fsdp-notes)

## Device Mesh

### [torch.distributed](https://pytorch.org/docs/stable/distributed.html)

### [Getting Started with DeviceMesh](https://pytorch.org/tutorials/recipes/distributed_device_mesh.html)

## Tensor Parallel (TP)

### [torch.distributed.tensor.parallel](https://pytorch.org/docs/stable/distributed.tensor.parallel.html)

### [Large Scale Transformer model training with Tensor Parallel (TP)](https://pytorch.org/tutorials/intermediate/TP_tutorial.html)

### [Tensor Parallelism](https://pytorch.org/docs/stable/distributed.tensor.parallel.html)

## Custom Extensions

### [Distributed Optimizers](https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer)

### [Distributed RPC Framework](https://pytorch.org/docs/stable/rpc.html)

### [Customize Process Group Backends Using Cpp Extensions](https://pytorch.org/tutorials/intermediate/process_group_cpp_extension_tutorial.html)

## Remote Procedure Call (RPC) distributed training

### [Getting Started with Distributed RPC Framework](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html)

### [RPC API documents](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html)

### [Implementing a Parameter Server Using Distributed RPC Framework](https://pytorch.org/tutorials/intermediate/#rpc_param_server_tutorial.html)

### [Implementing Batch RPC Processing Using Asynchronous Executions](https://pytorch.org/tutorials/intermediate/rpc_async_execution.html)

### [Combining Distributed DataParallel with Distributed RPC Framework](https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html)

### [Direct Device-to-Device Communication with TensorPipe RPC](https://pytorch.org/tutorials/recipes/cuda_rpc.html)

## [Introduction to Libuv TCPStore Backend](https://pytorch.org/tutorials/intermediate/TCPStore_libuv_backend.html)

## Pipeline Parallelism

### [Pipeline Parallelism](https://pytorch.org/docs/main/distributed.pipelining.html)

### [Introduction to Distributed Pipeline Parallelism](https://pytorch.org/tutorials/intermediate/pipelining_tutorial.html)

## [Shard Optimizer States with ZeroRedundancyOptimizer](https://pytorch.org/tutorials/recipes/zero_redundancy_optimizer.html)

## [Distributed Optimizer with TorchScript support](https://pytorch.org/tutorials/recipes/distributed_optim_torchscript.html)

## Checkpoint (DCP)

### [Getting Started with Distributed Checkpoint (DCP)](https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html)

### [Asynchronous Checkpointing (DCP)](https://pytorch.org/tutorials/recipes/distributed_async_checkpoint_recipe.html)

## AMP

### [Automatic Mixed Precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)

### [Automatic Mixed Precision examples](https://pytorch.org/docs/stable/notes/amp_examples.html)

### [Automatic Mixed Precision package - torch.amp](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)

TODO
